{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOTACINES\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicas de Python basico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.itexams.com/exam/PCAP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "while n < 4:\n",
    "    n += 1\n",
    "    print(n, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python siempre opera de derecha a izquierda.\n",
    "x = 0\n",
    "y = 2\n",
    "z = len(\"Python\")\n",
    "\n",
    "x = y > z # x = 2, z = 6\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Val = 1\n",
    "Val2 = 0\n",
    "\n",
    "Val = Val ^ Val2\n",
    "Val2 = Val ^ Val2\n",
    "\n",
    "Val = Val ^ Val2\n",
    "\n",
    "print(Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, y, x = 2, 1, 0\n",
    "x, z = z, y # x=2, z=1\n",
    "y = y - z # 1-1, luego y=0\n",
    "x, y, z = y, z, x\n",
    "print(x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = a ** 0\n",
    "if b < a + 1:\n",
    "    c = 1\n",
    "elif b == 1:\n",
    "    c = 2\n",
    "else:\n",
    "    c = 3\n",
    "print(a + b + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "while i > 0 :\n",
    "    i -= 3\n",
    "    print(\"*\")\n",
    "    if i <= 3:\n",
    "        break\n",
    "else:\n",
    "    print(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "for i in range(1, 4, 2):\n",
    "    print(\"*\") # 2\n",
    "\n",
    "# Example 2\n",
    "for i in range(1, 4, 2):\n",
    "    print(\"*\", end=\"\") # 1\n",
    "\n",
    "# Example 3\n",
    "for i in range(1, 4, 2):\n",
    "    print(\"*\", end=\"**\") # 1\n",
    "\n",
    "# Example 4\n",
    "for i in range(1, 4, 2):\n",
    "    print(\"*\", end=\"**\")\n",
    "print(\"***\") # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"20\"\n",
    "y = \"30\"\n",
    "print(x > y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Hello, Python!\"\n",
    "print(s[-14:15])\n",
    "# print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [\"A\", \"B\", \"C\", 2, 4]\n",
    "del lst[0:-2]\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = { 'a': 1, 'b': 2, 'c': 3 }\n",
    "for item in dict:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'python'\n",
    "for i in range(len(s)):\n",
    "    i = s[i].upper()\n",
    "print(s, end=\"\")\n",
    "# Esto pasa porque realmente no se esta cambiando a s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst = [i // i for i in range(0,4)]\n",
    "# sum = 0\n",
    "# for n in lst:\n",
    "#     sum += n\n",
    "# print(sum)\n",
    "# print(sum([0/0, 1/1, 2/2, 3/3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [[c for c in range(r)] for r in range(3)]\n",
    "for x in lst:\n",
    "    for y in x:\n",
    "        if y < 2:\n",
    "            print(y)\n",
    "            # print('*', end='')\n",
    "\n",
    "# No imprime numero de una lista vacia.  \n",
    "# print([[0], [0,1], [0,1,2]])\n",
    "# print('*****')\n",
    "# print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [2 ** x for x in range(0, 11)]\n",
    "print(2**10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst1 = \"12,34\"\n",
    "lst2 = lst1.split(',')\n",
    "print(len(lst1) < len(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a, b=0, c=5, d=1):\n",
    "    return a ** b ** c\n",
    "\n",
    "print(fun(b=2, a=2, c=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5\n",
    "f = lambda x: 1 + 2\n",
    "print(f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen():\n",
    "    lst = range(5)\n",
    "    for i in lst:\n",
    "        yield i*i\n",
    "\n",
    "for i in gen():\n",
    "    print(i, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "class CriticalError(Exception):\n",
    "    def __init__(self, message='ERROR MESSAGE A'):\n",
    "        Exception.__init__(self, message)\n",
    "\n",
    "raise CriticalError\n",
    "raise CriticalError(\"ERROR MESSAGE B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista=[False for i in range(1,10)]\n",
    "print(lista[-1:1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el logger\n",
    "logger = logging.getLogger('anotaciones')\n",
    "logger.setLevel(logging.DEBUG)  # Configurar nivel del logger\n",
    "\n",
    "# Limpiar los handlers previos (si hay)\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "# Crear handler para el archivo de log\n",
    "file_handler = logging.FileHandler('myapp.log', mode='w')\n",
    "file_handler.setLevel(logging.DEBUG)  # Nivel del archivo log\n",
    "file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(file_formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Crear handler para la consola\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)  # Nivel de la consola\n",
    "console_formatter = logging.Formatter('%(name)s - %(levelname)s: %(message)s')\n",
    "console_handler.setFormatter(console_formatter)\n",
    "logger.addHandler(console_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(f\"Memoria Fisica total (RAM): {round((psutil.virtual_memory().total)/1024**3, 2)} Gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.empty((1024, 1024, 1024, 3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1, 2], [3, 4]]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [\n",
    "        [1, 0, 0], \n",
    "        [0, 1, 0], \n",
    "        [0, 0, 1]\n",
    "    ],\n",
    "    [\n",
    "        [2, 0, 0], \n",
    "        [0, 2, 0], \n",
    "        [0, 0, 2]   \n",
    "    ]\n",
    "])\n",
    "\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones\n",
    "num_palabras = 3\n",
    "dimensionalidad = 4\n",
    "\n",
    "# Matrices aleatorias Q, K y V\n",
    "np.random.seed(0)  # Para reproducibilidad\n",
    "Q = np.random.rand(num_palabras, dimensionalidad)\n",
    "K = np.random.rand(num_palabras, dimensionalidad)\n",
    "V = np.random.rand(num_palabras, dimensionalidad)\n",
    "\n",
    "print(\"Q:\\n\", Q)\n",
    "print(\"\\nK:\\n\", K)\n",
    "print(\"\\nV:\\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producto punto entre Q y K^T\n",
    "producto_QK = np.dot(Q, K.T)\n",
    "print(\"\\nProducto QK^T:\\n\", producto_QK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Factor de escalado\n",
    "d_k = K.shape[1]  # dimensionalidad\n",
    "producto_QK_escalado = producto_QK / math.sqrt(d_k)\n",
    "print(\"\\nProducto QK^T escalado:\\n\", producto_QK_escalado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Aplicar softmax\n",
    "pesos_atencion = softmax(producto_QK_escalado)\n",
    "print(\"\\nPesos de Atención (Softmax aplicado):\\n\", pesos_atencion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salida de la atención\n",
    "salida_atencion = np.dot(pesos_atencion, V)\n",
    "print(\"\\nSalida de la Atención:\\n\", salida_atencion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atencion(Q, K, V):\n",
    "    producto_QK = np.dot(Q, K.T)\n",
    "    producto_QK_escalado = producto_QK / math.sqrt(K.shape[1])\n",
    "    pesos_atencion = softmax(producto_QK_escalado)\n",
    "    salida_atencion = np.dot(pesos_atencion, V)\n",
    "    return salida_atencion, pesos_atencion\n",
    "\n",
    "# Llamada a la función\n",
    "salida_atencion, pesos_atencion = atencion(Q, K, V)\n",
    "\n",
    "print(\"\\nSalida de la Atención:\\n\", salida_atencion)\n",
    "print(\"\\nPesos de Atención:\\n\", pesos_atencion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "num_cabezas = 2\n",
    "dim_cabeza = dimensionalidad // num_cabezas\n",
    "\n",
    "# Inicializar matrices para cada cabeza\n",
    "def inicializar_QKV(num_palabras, num_cabezas, dim_cabeza):\n",
    "    Q = np.random.rand(num_palabras, num_cabezas, dim_cabeza)\n",
    "    K = np.random.rand(num_palabras, num_cabezas, dim_cabeza)\n",
    "    V = np.random.rand(num_palabras, num_cabezas, dim_cabeza)\n",
    "    return Q, K, V\n",
    "\n",
    "Q, K, V = inicializar_QKV(num_palabras, num_cabezas, dim_cabeza)\n",
    "\n",
    "# Función de atención modificada para múltiples cabezas\n",
    "def multi_head_atencion(Q, K, V):\n",
    "    salidas_cabezas = []\n",
    "    for i in range(Q.shape[1]):  # Iterar sobre las cabezas\n",
    "        salida, _ = atencion(Q[:, i, :], K[:, i, :], V[:, i, :])\n",
    "        salidas_cabezas.append(salida)\n",
    "    # Concatenar las salidas de las cabezas\n",
    "    salida_concatenada = np.concatenate(salidas_cabezas, axis=-1)\n",
    "    return salida_concatenada\n",
    "\n",
    "# Llamada a la función\n",
    "salida_multi_head = multi_head_atencion(Q, K, V)\n",
    "print(\"\\nSalida de Multi-Head Attention:\\n\", salida_multi_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def frase_a_tokens(frase: str) -> List[str]:\n",
    "    # Usar una expresión regular para eliminar los símbolos especiales\n",
    "    frase_limpia = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", frase)\n",
    "    \n",
    "    # Las palabras se separan por espacios\n",
    "    lista_de_palabras: List[str] = frase_limpia.split()\n",
    "    \n",
    "    return lista_de_palabras\n",
    "\n",
    "def construccion_vocabulario(tokens):\n",
    "    # Las palabras deben ser unicas, es como construir un diccionario.\n",
    "    set_de_palabras_unicas = set(tokens)\n",
    "    # Ahora debo asignar un indice a cada una de las palabras de mi diccionario.\n",
    "    vocabulario: Dict[str, int] = {\n",
    "        palabra : indice for indice, palabra in enumerate(set_de_palabras_unicas)\n",
    "    }\n",
    "    return vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION_DEL_EMBEDDING: int = 5\n",
    "\n",
    "FRASE: str = 'El gato come pescado.'\n",
    "\n",
    "lista_de_tokens = frase_a_tokens(FRASE)\n",
    "vocabulario = construccion_vocabulario(lista_de_tokens)\n",
    "\n",
    "# Crear un embedding aleatorio para cada palabra\n",
    "embeddings = {palabra: np.random.rand(DIMENSION_DEL_EMBEDDING) for palabra in vocabulario}\n",
    "for palabra, vector in embeddings.items():\n",
    "    print(f\"Embedding de '{palabra}': {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir la matriz de embeddings de la secuencia\n",
    "X = np.array([embeddings[palabra] for palabra in lista_de_tokens])\n",
    "print(\"\\nMatriz de Embeddings (X):\\n\", X)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar matrices de peso para Q, K y V\n",
    "W_Q = np.random.rand(DIMENSION_DEL_EMBEDDING, X.shape[0])\n",
    "W_K = np.random.rand(DIMENSION_DEL_EMBEDDING, X.shape[0])\n",
    "W_V = np.random.rand(DIMENSION_DEL_EMBEDDING, X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular Q, K y V\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "print(\"\\nMatriz Q:\\n\", Q)\n",
    "print(\"\\nMatriz K:\\n\", K)\n",
    "print(\"\\nMatriz V:\\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros de atencion.\n",
    "DIMENSION_EMBEDDING: int = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTO: str = \"\"\"\n",
    "Quiero un gato, este come pescado. Me encantan los gatos\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    List,\n",
    "    Dict,\n",
    "    Any,\n",
    "    Tuple\n",
    ")\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Parámetros de atención.\n",
    "DIMENSION_EMBEDDING: int = 4\n",
    "\n",
    "TEXTO: str = \"\"\"\n",
    "Quiero un gato, este come pescado. Me encantan los gatos\n",
    "\"\"\"\n",
    "\n",
    "def tokenizador(texto: str) -> List[str]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Lo que hace es separar el texto.\n",
    "    Lo convierte en una lista de palabras, estas no tienen por que ser unicas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Limpiar texto de símbolos especiales y convertir a minúsculas.\n",
    "    texto_limpio = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", texto)\n",
    "    return texto_limpio.lower().split()\n",
    "\n",
    "def creador_de_vocabulario(lista_de_tokens: List[str]) -> Dict[str, int]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Crear vocabulario asignando un índice único a cada palabra.\n",
    "    Luego el resultado si contendra palabras unicas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Crear vocabulario asignando un índice único a cada palabra\n",
    "    return {palabra : indice for indice, palabra in enumerate(set(lista_de_tokens))}\n",
    "\n",
    "def creador_de_embeddings(\n",
    "    vocabulario: List[str],\n",
    "    dimension: int,\n",
    "    inicializacion_aleatoria: bool = True\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "    \n",
    "    # Crear embeddings aleatorios para cada palabra en el vocabulario.\n",
    "    if inicializacion_aleatoria:\n",
    "        embeddings: Dict[str, np.ndarray] = {palabra: np.random.rand(dimension) for palabra in vocabulario}\n",
    "    else:\n",
    "        raise ValueError('Aun no preparado.')\n",
    "    return embeddings\n",
    "\n",
    "def construccion_matriz_embeddings_frase(lista_de_tokens: List[str], diccionario_de_embeddings: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    # Construir la matriz de embeddings a partir de los tokens\n",
    "    return np.array([diccionario_de_embeddings[palabra] for palabra in lista_de_tokens])\n",
    "\n",
    "# Construir las matrices Q, K, V\n",
    "def construir_Q_K_V(matriz_embedded: np.ndarray, dim_proyeccion: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    # Inicializar las matrices de pesos aleatorios para Q, K, V\n",
    "    W_Q = np.random.rand(matriz_embedded.shape[1], dim_proyeccion)  # Pesos para Q\n",
    "    W_K = np.random.rand(matriz_embedded.shape[1], dim_proyeccion)  # Pesos para K\n",
    "    W_V = np.random.rand(matriz_embedded.shape[1], dim_proyeccion)  # Pesos para V\n",
    "    \n",
    "    # Multiplicar la matriz de embeddings por los pesos para obtener Q, K, V\n",
    "    Q = np.dot(matriz_embedded, W_Q)\n",
    "    K = np.dot(matriz_embedded, W_K)\n",
    "    V = np.dot(matriz_embedded, W_V)\n",
    "    \n",
    "    return Q, K, V\n",
    "\n",
    "def calcular_atencion(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n",
    "    # 1. Calcular el producto punto escalado entre Q y K^T\n",
    "    # Se escala por la raíz cuadrada de la dimensión de la proyección\n",
    "    d_k = K.shape[-1]\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # 2. Aplicar softmax a los scores para obtener las probabilidades de atención\n",
    "    softmax_scores = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # 3. Multiplicar los scores normalizados (atención) por V\n",
    "    atencion = np.dot(softmax_scores, V)\n",
    "    \n",
    "    return atencion\n",
    "\n",
    "def multi_head_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, num_heads: int) -> np.ndarray:\n",
    "    # Dividir las matrices en múltiples \"cabezas\"\n",
    "    d_k = Q.shape[-1] // num_heads\n",
    "    cabezas_atencion = []\n",
    "    \n",
    "    for _ in range(num_heads):\n",
    "        # Para cada cabeza, calcular atención usando partes de Q, K, V\n",
    "        Q_head = Q[:, :d_k]\n",
    "        K_head = K[:, :d_k]\n",
    "        V_head = V[:, :d_k]\n",
    "        \n",
    "        # Calcular la atención para esta cabeza\n",
    "        atencion_cabeza = calcular_atencion(Q_head, K_head, V_head)\n",
    "        cabezas_atencion.append(atencion_cabeza)\n",
    "    \n",
    "    # Concatenar los resultados de todas las cabezas\n",
    "    atencion_concatenada = np.concatenate(cabezas_atencion, axis=-1)\n",
    "    \n",
    "    return atencion_concatenada\n",
    "\n",
    "def normalizacion_layer(x: np.ndarray) -> np.ndarray:\n",
    "    # Normalizar la matriz (Layer Norm) por sus medias y varianzas\n",
    "    return (x - np.mean(x, axis=-1, keepdims=True)) / np.sqrt(np.var(x, axis=-1, keepdims=True) + 1e-6)\n",
    "\n",
    "def feed_forward(x: np.ndarray, dim_ff: int) -> np.ndarray:\n",
    "    # Capa feed-forward simple\n",
    "    W1 = np.random.rand(x.shape[-1], dim_ff)\n",
    "    W2 = np.random.rand(dim_ff, x.shape[-1])\n",
    "    \n",
    "    return np.dot(np.maximum(0, np.dot(x, W1)), W2)  # ReLU + Linear\n",
    "\n",
    "def agregar_positional_encoding(embeddings: np.ndarray, max_len: int) -> np.ndarray:\n",
    "    d_model = embeddings.shape[1]\n",
    "    pe = np.zeros((max_len, d_model))\n",
    "    for pos in range(max_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            pe[pos, i] = np.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "            pe[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i+1))/d_model)))\n",
    "    return embeddings + pe[:embeddings.shape[0], :]\n",
    "\n",
    "def add_and_norm(x: np.ndarray, sublayer_output: np.ndarray) -> np.ndarray:\n",
    "    return normalizacion_layer(x + sublayer_output)\n",
    "\n",
    "def proyeccion_lineal(y: np.ndarray, vocab_size: int) -> np.ndarray:\n",
    "    W = np.random.rand(y.shape[-1], vocab_size)\n",
    "    return np.dot(y, W)\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def calcular_atencion_enmascarada(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "    # Calcular la atención normal\n",
    "    d_k = K.shape[-1]\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Aplicar la máscara para impedir ver posiciones futuras\n",
    "    scores = scores + (mask * -1e9)  # Añadir un valor muy bajo donde hay máscara\n",
    "    \n",
    "    # Softmax\n",
    "    softmax_scores = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # Multiplicar los scores normalizados por V\n",
    "    atencion = np.dot(softmax_scores, V)\n",
    "    \n",
    "    return atencion\n",
    "\n",
    "def generar_texto(modelo, texto_inicial: str, vocabulario: Dict[str, int], vocabulario_inverse: Dict[int, str], max_len: int) -> str:\n",
    "    # Convertir el texto inicial en embeddings\n",
    "    tokens = tokenizador(texto_inicial)\n",
    "    \n",
    "    # Inicializar la secuencia generada con los tokens del texto inicial\n",
    "    secuencia_generada = tokens[:]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # Crear embeddings para la secuencia generada hasta el momento\n",
    "        lista_de_tokens = secuencia_generada\n",
    "        \n",
    "        # Volver a crear los embeddings y aplicar el modelo\n",
    "        diccionario_de_embeddings = creador_de_embeddings(vocabulario, DIMENSION_EMBEDDING)\n",
    "        matriz_embedded = construccion_matriz_embeddings_frase(lista_de_tokens, diccionario_de_embeddings)\n",
    "        matriz_embedded = agregar_positional_encoding(matriz_embedded, len(lista_de_tokens))\n",
    "        \n",
    "        Q, K, V = construir_Q_K_V(matriz_embedded, DIMENSION_EMBEDDING)\n",
    "        atencion = multi_head_attention(Q, K, V, num_heads=4)\n",
    "        atencion_normalizada = add_and_norm(matriz_embedded, atencion)\n",
    "        salida_feed_forward = feed_forward(atencion_normalizada, DIMENSION_EMBEDDING * 2)\n",
    "        salida_final = add_and_norm(atencion_normalizada, salida_feed_forward)\n",
    "        \n",
    "        logits = proyeccion_lineal(salida_final, len(vocabulario))\n",
    "        probabilidades = softmax(logits)\n",
    "\n",
    "        # Obtener el índice de la palabra con la mayor probabilidad\n",
    "        next_word_id = np.argmax(probabilidades[-1])\n",
    "        \n",
    "        # Convertir el id de vuelta a palabra usando el vocabulario inverso\n",
    "        next_word = vocabulario_inverse[next_word_id]\n",
    "        \n",
    "        # Añadir la palabra generada a la secuencia\n",
    "        secuencia_generada.append(next_word)\n",
    "        \n",
    "        # Si la palabra generada es un token de fin, detener el ciclo\n",
    "        if next_word == \"<EOS>\":\n",
    "            break\n",
    "    \n",
    "    return \" \".join(secuencia_generada)\n",
    "\n",
    "def calcular_perdida(logits: np.ndarray, etiquetas: np.ndarray) -> float:\n",
    "    # Usar la entropía cruzada para calcular la pérdida\n",
    "    log_probabilidades = np.log(softmax(logits))\n",
    "    loss = -np.sum(etiquetas * log_probabilidades)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def main(texto: str, dimenson_embedding: int, dimenson_de_proyeccion: int) -> Any:\n",
    "    # 1. Tokenizar el texto\n",
    "    lista_de_tokens: List[str] = tokenizador(texto)\n",
    "    \n",
    "    # 2. Crear el vocabulario a partir de los tokens\n",
    "    vocabulario: Dict[str, int] = creador_de_vocabulario(lista_de_tokens)\n",
    "    \n",
    "    # 3. Crear el vocabulario inverso para las predicciones\n",
    "    vocabulario_inverse = {indice: palabra for palabra, indice in vocabulario.items()}\n",
    "    \n",
    "    # 4. Generar texto con el modelo actual\n",
    "    texto_generado = generar_texto(main, texto, vocabulario, vocabulario_inverse, max_len=10)\n",
    "    \n",
    "    return texto_generado\n",
    "\n",
    "# Ejecutar el código principal\n",
    "resultado = main(TEXTO, DIMENSION_EMBEDDING, DIMENSION_EMBEDDING)\n",
    "\n",
    "# Mostrar el texto generado\n",
    "print(\"Texto generado:\")\n",
    "print(resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definir el modelo\n",
    "modelo = MiModelo()\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterio = nn.MSELoss()\n",
    "optimizador = optim.SGD(modelo.parameters(), lr=0.01)\n",
    "\n",
    "# Datos de entrada y etiquetas de ejemplo\n",
    "entradas = torch.randn(10, 10)\n",
    "etiquetas = torch.randn(10, 5)\n",
    "\n",
    "# Ciclo de entrenamiento\n",
    "for epoch in range(100):\n",
    "    optimizador.zero_grad()    # Limpiar gradientes\n",
    "    salida = modelo(entradas)  # Forward pass\n",
    "    perdida = criterio(salida, etiquetas)  # Calcular la pérdida\n",
    "    perdida.backward()         # Backpropagation\n",
    "    optimizador.step()         # Actualizar los pesos\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Pérdida: {perdida.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praticando: Attention is all you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llamy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL: str = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "texto = llamy.obtener_texto_desde_url(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(texto)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
